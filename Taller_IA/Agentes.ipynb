{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "t.Tensor([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = t.nn.Sequential(\n",
    "    t.nn.Linear(10, 150),\n",
    "    t.nn.ReLU(),\n",
    "    t.nn.Linear(150, 4),\n",
    "    t.nn.ReLU()\n",
    ")\n",
    "loss_fn = t.nn.MSELoss()\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     y_hat \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m      3\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(y_hat, y_true)\n\u001b[1;32m      4\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "for step in range(100):\n",
    "    y_hat = model(x)\n",
    "    loss = loss_fn(y_hat, y_true)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Linear\n",
    "\n",
    "class mi_red(Module):\n",
    "    def __init__(self):\n",
    "        super(mi_red, self).__init__()\n",
    "        self.fc1 = Linear(784, 50)\n",
    "        self.fc2 = Linear(50, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        return X\n",
    "\n",
    "model = mi_red()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "arms = 10\n",
    "N, D_in, H, D_out = 1, arms, 100, arms\n",
    "loss_fn = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, epochs=5000, learning_rate=0.01):\n",
    "    cur_state = torch.tensor(one_hot(arms, env.get_state()))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    rewards = []\n",
    "    for i in range(epochs):\n",
    "        y_pred = model(cur_state)\n",
    "        av_softmax = softmax(y_pred.data.numpy(), tau=2.0)\n",
    "        av_softmax /= av_softmax.sum()\n",
    "        choice = np.random.choice(arms, p=av_softmax)\n",
    "        cur_reward = env.choose_arm(choice)\n",
    "        one_hot_reward = y_pred.data.numpy().copy()\n",
    "        one_hot_reward[choice] = cur_reward\n",
    "        reward = torch.Tensor(one_hot_reward)\n",
    "        rewards.append(cur_reward)\n",
    "        loss = loss_fn(y_pred, reward)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cur_state = torch.Tensor(one_hot(arms, env.get_state()))\n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocodigo\n",
    "def get_update_q_value(old_q_value, reward, state, step_size, discount):\n",
    "    term2 = (reward + discount * max([Q(state, action) for action in actions]))\n",
    "    term2 -= old_q_value\n",
    "    term2 = step_size * term2\n",
    "    return (old_q_value + term2)\n",
    "\n",
    "# El valor Q en el momento t se actualiza para que sea el valor Q previsto \n",
    "# actual más la cantidad de valor que esperamos en el futuro, dado que \n",
    "# jugamos de forma óptima a partir de nuestro estado actual."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
